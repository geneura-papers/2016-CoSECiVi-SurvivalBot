%\documentclass[runningheads,a4paper]{llncs}
\documentclass{llncs}

\usepackage[latin1]{inputenc}
%\usepackage[spanish]{babel}
\usepackage[spanish, es-tabla]{babel} % Denominación de tablas en lugar de 'cuadros'
\usepackage{graphicx,color,longtable,multirow,times,amsmath,url}
\usepackage[dvips]{epsfig}
\usepackage{textcomp}
\usepackage{verbatim}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{fancyvrb}
\usepackage{subfigure}
\usepackage{listings}


% \renewcommand{\tablename}{Tabla} 

\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\providecommand{\tabularnewline}{\\}

\begin{document}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{Sólo puede quedar uno: \\ Evolución de Bots para RTS 
basada en supervivencia}
% Antonio - es la frase de Los Inmortales. :D


% a short form should be given in case it is too long for the running head
\titlerunning{Sólo puede quedar uno}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%

\author{A. Fern\'andez-Ares \inst{1} \and A.M. Mora \inst{2} \and
  P. Garc\'ia-S\'anchez  \inst{1} \and P.A. Castillo \and J.J. Merelo \inst{1}}
%\thanks{NoInstitute}}

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{Depto. de Arquitectura y Tecnología de Computadores\\ 
%ETSIIT-CITIC, Universidad de Granada, España.\\
\and
Depto. de Lenguajes y Sistemas Informáticos\\
ETSIIT-CITIC, Universidad de Granada, España.\\
\{antares,amorag,pablogarcia,pacv,jmerelo\}@.ugr.es}



%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

\maketitle

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   ABSTRACT   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{abstract}
Este artículo propone un algoritmo evolutivo para optimizar el comportamiento de bots (NPCs) que no requiere de una función de fitness explícita, usando en su lugar combates por pares (a modo de ``justa'') en los que sólo uno de los contendientes sobrevivirá. Este proceso hará las veces de mecanismo de selección del algoritmo, en el que sólo los ganadores pasarán a la siguiente generación del mismo.
Se ha utilizado un algoritmo de Programación Genética, diseñado para generar motores de comportamiento para bots del conocido RTS Planet Wars. 
Este método tiene dos objetivos principales: en primer lugar, paliar el efecto que la naturaleza ``ruidosa'' de la función de fitness añade a la evaluación y, en segundo lugar, generar bots más generales (menos especializados) que los que se obtienen mediante algoritmos evolutivos en los que se usa siempre un contendiente común para evaluar los individuos.
Más aún, la omisión de un proceso de evaluación explícito reduce el número de combates necesarios para evolucionar, lo que reduce a su vez el tiempo de cómputo del algoritmo.
Los resultados demuestran que el método converge y que es menos sensible al ruido que otros métodos más tradicionales. Además de esto, con este algoritmo se obtienen bots muy competitivos en comparación con otros bots de la literatura.

%
%\keywords{Videogames, RTS, evolutionary algorithms, joust selection, survival, co-evolution}
\end{abstract}

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   INTRODUCTION   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Introducción y descripción del problema}
\label{sec:intro}


Los Algoritmos Evolutivos (AEs) han sido aplicados a juegos durante mucho tiempo, a pesar de que la evaluación de posibles estrategias generadas es \textit{ruidosa} \cite{Genebot_JCST} en el sentido de que existe una incertidumbre inherente sobre el verdadero \textit{fitness} o calidad del \textit{Bot} (NPC) que se esté evaluando.
Esto es debido a que dicha evaluación depende de varios factores estocásticos: reglas del juego y estado actual,  el comportamiento de los rivales o el estado inicial del juego, dado que todos ellos tendrán influencia en el resultado que pueda obtener el agente. 


{\em Planet Wars}, el RTS (\textit{Real-Time Strategy game}) que se propuso como banco de pruebas para la celebración del Google AI Challenge 2010\footnote{\url{http://planetwars.aichallenge.org/}}, obviamente, también presenta este problema. Este juego ha sido utilizado por numerosos autores para el estudio de técnicas de Inteligencia Computacional (IC) en RTSs, tales como la generación automática u optimización de bots o mapas del juego \cite{Genebot_CEC11,ziolko2012automatic,Genebot_CIG2012,LaraMaps14}. 

A modo de resumen, el objetivo del jugador es conquistar los planetas neutrales y los que posee el enemigo en un simulador ``espacial''. Cada jugador poseerá planetas (recursos) que producen naves (unidades) en función de una tasa de crecimiento. El jugador deberá controlar su flota de naves (sólo podrá actúar sobre ellas), enviándolas a otros planetas para conquistarlos o reforzarlos (en caso de pertenecer al jugador previamente). En el primer caso, dichas naves se estrellarán literalmente contra los planetas, reduciendo el número de naves o puntos necesarios para conquistarlos (en una relación de 1 por nave estrellada). Si este número llega a 0 el jugador pasará a dominar ese planeta, que generará naves para él a partir de entonces.
El ganador será aquel que posea todos los planetas al final de la partida o aquel al que le resten unidades (si un jugador se queda sin naves pierde la partida).

Existen ciertas restricciones, como un límite de tiempo de un segundo para tomar decisiones, por lo que se podrían considerar {\em turnos}\footnote{Aunque usemos este término, se considera que el juego transcurre en tiempo real.}; o también la imposibilidad de guardar y considerar acciones de turnos anteriores (memoria de acciones realizadas). 

La Figura \ref{figura:PlanetWars1} muestra una captura de pantalla del juego.

\begin{figure}[htb]
\tiny
\begin{center}
  \epsfig{file=./imags/planet_wars_battle.eps,width=5cm}
\end{center}
\caption{Captura de la simulación de un estado temprano del juego Planet Wars.
Los planetas blancos pertenecen al jugador, los planetas grises oscuros
pertenecen al oponente y los planetas grises claros no pertenecen
aún a ningún jugador. Los triángulos representas flotas de naves.
Los números (tanto en planetas como en flotas) representan el número
de naves que lo componen. El tamaño de los planetas se refiere a la tasa de crecimiento (número de naves que genera por turno) que tiene asociada, de modo que será mayor cuanto más grande sea el planeta.}
\label{figura:PlanetWars1}
\end{figure}

Un bot de PlanetWars se encargará de controlar toda la flota, realizando las acciones pertinentes sobre las naves para doblegar la flota enemiga y conquistar todos los planetas en liza.

Este juego presenta el problema mencionado antes en el cálculo del fitness \cite{Genebot_JCST}. De modo que, como medida habitual, se realizan diversos combates para evaluar a cada individuo (en mapas diferentes y contra diferentes enemigos), de modo que su fitness se obtiene como una media o sumatoria de los resultados obtenidos.
De esta forma, idealmente, se obtendría una medida más precisa (menos ruidosa) de la calidad de cada individuo. Pero, en realidad, no es aún una solución totalmente fiable dado que depende de diversos valores del propio bot, como el número de victorias obtenidas o de naves generadas en las batallas; así como de otros relacionados con el rendimiento del rival, que podría ser también un bot.

De modo que, incluso si obtuviésemos una evaluación estadísticamente significativa, el calculo del fitnes podría incluir una tendencia relacionada con el oponente seleccionado. Este problema está relacionado con el sobre-entrenamiento de la población para enfrentarse a un rival específico, es decir, los individuos aprenden a jugar excesivamente bien contra él y obtienen peores resultados contra otros enemigos \cite{Genebot_JCST,wilcoxon:ga}.

Este artículo propone un AE co-evolutivo (AEC) \cite{Paredis_CEA} para mejorar la IA de bots de Planet Wars por medio de una evaluación implícita del fitness. Ésta se basará en la \textit{supervivencia de los individuos}.
Para ello, el proceso de selección se transformará en un \textit{torneo} (o \textit{justa}, para diferenciar el término del clásico torneo de los AEs), en el que sólo los ganadores sobrevivirán y pasarán a ser individuos (padres) de la siguiente generación. De este modo se evita el cálculo del fitness y, por lo tanto, la influencia del ruido que ésta función introduce se reduce.
Además, con este enfoque se evitan algunos de los parámetros de configuración del algoritmo, como el número de combates o los rangos para las puntuaciones, así como la consideración de un rival en particular para combatir.

Este modelo es más cercano al proceso real de selección natural \cite{darwin1859}, en el que sólo los mejores sobreviven, de lo que lo están los AEs canónicos.
De forma que el método propuesto se podría denominar un \textit{algoritmo co-evolutivo competitivo} \cite{Rosin_competitive_coevolution,YeoKeun_tournament-based_CoEA}, en el que el fitness (implícito aquí) depende de una competición contra otros individuos.

La propuesta se ha implementado como un algoritmo de Programación Genética (PG) \cite{GP_Koza92}, debido a la flexibilidad que este modelo ofrece respecto a los Algoritmos Genéticos (AGs) \cite{GAs_Goldberg89}, es decir, con PG se pueden crear nuevas reglas de comportamiento, mientras que el AG estaría centrado en la optimización de un conjunto ya existente. Además, esta técnica la hemos aplicado con buenos resultados en trabajos anteriores en esta misma línea \cite{GarciaGP14,EsparciaGP2013}.

Dado que el algoritmo se ejecuta en conjunción con Planet Wars, la justa se ha modelado como un combate en el juego.
El ganador de la batalla pasa a la siguiente generación como padre de los siguientes bots, el perdedor se elimina de la población.
Además, la consideración de todos los individuos como oponentes, en lugar de uno sólo, hace el entrenamiento (la evolución) más generalista, lo que producirá bots capaces de enfrentarse con éxito a un rango más amplio de rivales (aprenderán diversas estrategias).

Se han realizado varios experimentos, a fin de medir la convergencia en la evolución, así como la influencia del ruido en la misma, y se han comparado los resultados con los obtenidos por otros algoritmos y bots de la literatura.
Nuestro objetivo es comprobar si el método propuesto es válido para evaluar individuos, si resulta menos sensible a la influencia del ruido y si los bots generados son suficientemente competitivos.


%The paper is structured as follows. The background section reviews related work regarding the scope (videogames) and the implementation (EAs with no fitness computation or different selection mechanisms). It also describes the problem enclosed in the Planet Wars game.
%Section \ref{sec:survival_bots} presents the proposed method, termed {Survival Bot}.
%The experiments analyzing the evolution, noise and obtained bots are described and discussed in Section \ref{sec:experiments}. Finally, the conclusions and future lines of research are presented in Section \ref{sec:conclusions}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  STATE OF THE ART  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Conceptos preliminares y estado del arte}
\label{subsec:soa}

La evolución de motores de IA para controlar NPCs/Bots se ha convertido en una técnica muy exitosa en el entorno de los videojuegos. Existen dos enfoques principales: partir de un conjunto de reglas, cuyos parámetros o variables son optimizados \textit{off-line} (antes de la partida real) por medio de AGs \cite{Genebot_CEC11,genebot-evo12,Genebot_CIG2012,CarSetup,book:evolvingtowin}; o bien usar PG para crear automáticamente el conjunto de reglas que compondrán dicho motor de IA partiendo de una serie de condiciones y acciones (antecedentes y consecuentes de las reglas) \cite{GarciaGP14,EsparciaGP2013}.

El problema en ambos casos suele ser la consideración de un bot específico para la evaluación de los potenciales individuos o la definición de una función de fitness que realmente pueda valorar el rendimiento de cada bot en la batalla, atendiendo, normalmente, a diversos factores.

Una posible forma de evitar estos problemas es el uso de AECs, en los que el comportamiento de un individuo depende del de otros individuos de la población.
En el ámbito de los videojuegos, los AECs se empezaron a aplicar en juegos de tablero como Backgammon \cite{Pollack_Backgammon98}, o Go \cite{Runarsson_Go2005} hace muchos años.
Además, en años posteriores, este enfoque se ha aplicado a videojuegos, dentro de la rama de IC. Por ejemplo Togelius et al. \cite{Togelius_Cars2007} estudiaron los efectos de la co-evolución en una población de controladores para un juego de carreras de coches; Cook et al. \cite{Cook_Platforming2012} presentaron una propuesta co-evolutiva para el diseño automático de niveles de un juego de plataformas; y recientemente, Cardona et al. \cite{Cardona_MSPacman2013} experimentaron con el rendimiento de un algoritmo competitivo para la evolución simultánea de controladores para Ms. PacMan y el grupo de Fantasmas simultáneamente.

Nuestro trabajo también se ha enfocado como un método co-evolutivo, que es competitivo en la selección de individuos para ser padres de la siguiente generación y, a su vez, cooperativo puesto que todos los individuos forman parte del mismo proceso evolutivo.

Centrándonos en los trabajos dentro del ámbito de los RTSs, Livingstone \cite{Livinstone_RTS2005} comparó diferentes modelos de AI y propuso un entorno co-evolutivo de generación de los mismos, considerando diferentes niveles de AI (nivel de comandante, nivel de unidades, etc), de modo que era un enfoque cooperativo. El que se propone aquí se diferencia también en que el nivel de AI que se evoluciona es el mismo para todos los individuos.

Finalmente, Nogueira et al. \cite{Nogueira_HoF2013} consideraron en una publicación reciente el uso de la llamada \textit{Hall of Fame}, es decir, un conjunto de rivales que componen la élite de los oponentes hasta el momento, para evolucionar bots para el RTS {\em RobotWars}. Los mismos autores aplicaron una versión de dicho enfoque a Planet Wars \cite{NogueiraCoevolutionary14}. 
En ella, describieron un algoritmo de aprendizaje similar al que proponemos, pero centrado en un subconjunto de individuos (la élite), que creemos que podría tener un efecto negativo en cuanto a la capacidad de generalización de los bots.
Además, los autores aplican una función de fitness que depende de diversos parámetros, varios combates y medidas de puntuación adicionales.

La propuesta de este trabajo implementa un enfoque co-evolutivo basado en supervivencia, que evita el uso de un función de fitness explícita. El objetivo es intentar minimizar el efecto del ruido que introduciría dicha función \cite{Genebot_JCST}. Además, se evita el uso de parámetros adicionales y el uso de un rival (o rivales) específico en los combates que pudiera llevar a un sobre-entrenamiento contra él (o ellos).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   SURVIVAL BOT  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Survival Bots}
\label{sec:survival_bots}


Esta sección describe el algoritmo propuesto para generar bots llamados \textit{SurvivalBots}. En él se combina un algoritmo de PG \cite{GP_Koza92} con diferentes políticas de selección y reemplazo, basadas en la supervivencia de los mejores y con un enfoque co-evolutivo.

% ----------------------------------------------------------------------------

\subsection{Generación de Bots mediante PG}
\label{subsec:generationgp}

El algoritmo basado en Programación Genética (llamado {\em GPBot}) evoluciona un conjunto de parámetros que modela un árbol de decisión. Durante la evolución cada individuo de la población (un árbol) se evalúa. Para esto, el árbol que modela el motor de comportamiento de un agente, es colocado en un mapa en una partida de Planet Wars. Dependiendo de los resultados obtenidos, el agente (individuo) obtiene un valor fitness que se usa durante el proceso evolutivo.

Durante cada turno de la partida el árbol decidirá la mejor estrategia a seguir, seleccionando por cada planeta un objetivo y un porcentaje de naves a enviar. Estos árboles de decisión son árboles binarios de expresiones compuestas por dos diferentes tipos de nodos:

\begin{itemize}
\item {\em Decisión}: una expresión lógica formada por una variable, el operador $<$, y un número entre 0 y 1. Es el equivalente al concepto  ``primitiva'' en el campo de la PG.
\item {\em Acción}: una hoja del árbol (o sea, un ``terminal''). Cada decisión es el nombre del método a llamar del planeta que ejecuta el árbol. Este método indica a qué planeta enviar un porcentaje de las naves disponibles (de 0 a 1).
\end{itemize}

Las decisiones, definidas por un experto humano, se basan en los valores de las distintas \textit{variables} (también definidas por el experto) que son computadas considerando algunas otras variables del juego.


\begin{itemize}
\item {\em myShipsEnemyRatio}: Relación entre las naves del jugador y las naves del enemigo.
\item {\em myShipsLandedFlyingRatio}: Relación entre las naves del jugador que vuelan y están aterrizadas.
\item {\em myPlanetsEnemyRatio}: Relación entre el número de planetas del jugador y del enemigo.
\item {\em myPlanetsTotalRatio}:Relación entre el número de planetas del jugador y del total (incluyendo los del enemigo y los neutrales).
\item {\em actualMyShipsRatio}: Relación entre el número de naves en el planeta específico que evalúa el árbol y el total de naves del jugador.
\item {\em actualLandedFlyingRatio}: Relación entre el número de naves aterrizadas y volando desde el planeta específico que evalúa el árbol, y el total de naves del jugador.
\end{itemize}

Finalmente, las posibles  \textit{acciones}, según conocimiento experto, son:

\begin{itemize}
\item {\em Attack Nearest (Neutral|Enemy|NotMy) Planet}: El objetivo es el planeta más cercano. \textit{NotMy} se refiere a un planeta que no pertenezca al jugador, es decir, uno de los otros dos: enemigo o neutral.
\item {\em Attack Weakest (Neutral|Enemy|NotMy) Planet}: El objetivo es el planeta con menos naves.
\item {\em Attack Wealthiest (Neutral|Enemy|NotMy) Planet}: El objetivo es el planeta con más tasa de crecimiento.
\item {\em Attack Beneficial (Neutral|Enemy|NotMy) Planet}: El objetivo es el planeta más beneficioso, es decir, el que tiene mayor tasa de crecimiento dividido por el número de naves que alberga.
\item {\em Attack Quickest (Neutral|Enemy|NotMy) Planet}: El objetivo es el planeta más fácil de conquistar: el menor producto entre la distancia del planeta que ejecuta el árbol y el número de naves en el planeta objetivo.
\item {\em Attack (Neutral|Enemy|NotMy) Base}: El objetivo es el planeta con más naves (es decir, la base).
\item {\em  Attack Random Planet}. Atacar un planeta aleatorio.
\item {\em Reinforce Nearest Planet}: Reforzar el planeta más cercano al que ejecuta el árbol.
\item {\em Reinforce Base}: Reforzar al planeta con más naves del jugador.
\item {\em Reinforce Wealthiest Planet}: Reforzar al planeta del jugador con mayor tasa de crecimiento.
\item {\em Do nothing}. No hacer nada.

\end{itemize}

Un ejemplo de un árbol de decisión posible se muestra a continuación. Este ejemplo tiene un total de 5 nodos, con dos decisiones y tres acciones, con una profundidad de tres niveles.

\begin{verbatim}

if(myShipsLandedFlyingRatio < 0.796)
   if(actualMyShipsRatio < 0.201)
      attackWeakestNeutralPlanet(0.481);
   else
      attackNearestEnemyPlanet(0.913);
else
   attackNearestEnemyPlanet(0.819);

\end{verbatim}\\\\

El comportamiento del bot se explica en el Algoritmo  \ref{alg:turn}.

\begin{algorithm}[ht]
\begin{algorithmic}


\STATE \textit{//Al principio de la ejecución el agente recibe el árbol}
\STATE árbol $\leftarrow$ leerÁrbol()
\WHILE{el juego no termine}
	\STATE // iniciar el turno
	\STATE calcularPlanetasGlobales() \textit{// p.e. Base o Base Enemiga}
	\STATE calcularRatiosGlobales() \textit{// p.e. myPlanetsEnemyRatio }
	\FOR{cada p en planetas del jugador}
		\STATE calcularPlanetasLocales(p) \textit{// p.e. NearestNeutralPlanet a p}
		\STATE calcularRatiosDePlanetas(p) \textit{//p.e. actualMyShipsRatio}
		\STATE ejecutarÁrbol(p, árbol)  \textit{// Enviar un porcentaje de naves al destino}
   \ENDFOR
\ENDWHILE

\end{algorithmic}
\caption{Pseudocódigo del agente propuesto. El mismo árbol se ejecuta durante toda la ejecución del agente.}
\label{alg:turn}
\end{algorithm}

% ----------------------------------------------------------------------------

\vspace{-0.7cm}
\subsection{Selección por supervivencia}

El algoritmo presentado en la sección anterior se ha combinado con una \textit{evaluación implícita del fitness} que se ocupará de la selección y el reemplazo. Dicha evaluación es, en esencia, un combate entre individuos en un determinado mapa. Hemos denominado a este enfrentamiento \textit{justa} (para diferenciarlo del clásico torneo de los AEs).
De forma que la selección de padres para la siguiente generación se consigue mediante la resolución de estos combates, siendo el ganador (el superviviente) el elegido para continuar en la población y siendo el perdedor eliminado de la misma.

De esta forma, el proceso de selección intenta fomentar la supervivencia de los mejores individuos, paliando en cierta medida el ruido que añaden las funciones de evaluación explícitas (o numéricas) \cite{Genebot_JCST}. De modo que los individuos que no son capaces de ganar un combate son fuertemente penalizados y eliminados de la población.
Aún así, seguirá habiendo ruido, el cuál es intrínseco al problema en sí. Es decir, un individuo con peores condiciones que otro podría ganar un combate por causas ajenas a su buen rendimiento, como errores del rival o condiciones más favorables en el desarrollo del juego. Aunque Planet Wars parte siempre de escenarios totalmente equivalentes para los contendientes.
En cualquier caso, la presencia de ruido permitirá, a su vez, añadir diversidad a la población, lo cual siempre es beneficioso en un algoritmo de optimización combinatoria como este.

En este enfoque hablamos de ``iteración'' como sinónimo de generación, dado que no se trata de un proceso evolutivo clásico, como se explicará más adelante.

% --------------------------------------------------------------------------

\subsection{Reemplazo de los perdedores}
\label{subsec:replacement}

Hemos implementado la política de reemplazo del enfoque estacionario clásico de los AEs \cite{Genitor_whitley}. De forma que una gran parte de la población pasa a la siguiente generación y otra parte es sustituida. Este enfoque pretende fomentar la explotación en la búsqueda, a fin de mejorar el factor de convergencia del método. El objetivo es reducir la exploración que es más sensible en un espacio de búsqueda ruidoso como este.

De modo que el algoritmo propuesto realiza dos combates (justas) por generación. Los contendientes se seleccionan de forma aleatoria entre los individuos de la población, asegurando simplemente que no se eligen los mismos bots para los dos combates.

Los dos ganadores pasan a ser los padres de la \textit{operación de cruce}, que genera dos nuevos hijos, que son además mutados e incluidos en la población en el lugar de los individuos que hayan perdido el combate.
Hemos considerado cruce de sub-árboles y mutación a nivel de nodo, dado que con ellos obtuvimos buenos resultados en trabajos previos \cite{EsparciaGP2013}.

Este enfoque presenta un componente de aleatoriedad mayor que el AE estacionario clásico, debido a la falta de una función de fitness que asigne un valor representativo a cada individuo. La selección aleatoria entre todos los individuos aumenta la probabilidad de que los malos individuos salgan de la población, lo que reduciría también el ruido presente en ésta.
Este será un factor muy relevante, como se demostrará en los experimentos realizados.

El Algoritmo \ref{alg:completealgorithm} muestra la combinación del algoritmo de PG propuesto con la evaluación implícita del fitness mencionada y los mecanismos de selección y reemplazo.


\begin{algorithm}[htb]
\begin{algorithmic}

\STATE poblacion $\leftarrow$ inicializarPoblacion()
\WHILE{criterio de terminación no cumplido}

    \STATE descendientes,perdedores,seleccionados $\leftarrow$ \{\}
    \STATE {\em /* Se eligen dos contendientes aleatorios para la justa */}
    \STATE contendientes $\leftarrow$ seleccionarContendientes(poblacion)
    \STATE {\em /* Los contendientes luchan y se obtiene un ganador y un perdedor */} 
    \STATE ganador1,perdedor1 $\leftarrow$ justa(contendientes)
    \STATE {\em /* Los bots selecionados no participan en más combates */}
    \STATE seleccionados $\leftarrow$ seleccionados + ganador1 + perdedor1
    \STATE {\em /* Contendientes de la segunda justa */}
    \STATE contendientes $\leftarrow$ selectContenders(poblacion)
    \STATE {\em /* Los contendientes luchan y se obtiene un ganador y un perdedor */}
    \STATE ganador2,perdedor2 $\leftarrow$ justa(contendientes)
    \STATE seleccionados $\leftarrow$ seleccionados + ganador2 + perdedor2
    \STATE {\em /* Se guarda referencia a los perdedores */}
    \STATE perdedores $\leftarrow$ perdedores + perdedor1 + perdedor2
    \STATE {\em /* PROCESO EVOLUTIVO */}
    \STATE hijo1,hijo2 $\leftarrow$ cruce(ganador1,ganador2);
    \STATE hijo1,hijo2 $\leftarrow$ mutacion(hijo1,hijo2)
    \STATE descendientes $\leftarrow$ descendientes + hijo1 + hijo2
    \STATE {\em /* Reemplazo de perdedores */}
    \STATE poblacion $\leftarrow$ poblacion - perdedores
    \STATE poblacion $\leftarrow$ poblacion + descendientes

\ENDWHILE

\end{algorithmic}
\caption{Pseudocode of the proposed SurvivalBot.}
\label{alg:completealgorithm}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   EXPERIMENTS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Experimentos y resultados}
\label{sec:experiments}

Se han realizado varios experimentos para estudiar diferentes aspectos de la propuesta, pero debemos señalar que el principal objetivo no es la generación de los mejores bots (los más competitivos) posibles a toda costa, sino, en primera instancia, probar la validez de este método co-evolutivo basado en selección por supervivencia o justa. De modo que comprobaremos que el algoritmo converge y que el ruido tiene una influencia menor que en otros casos. Posteriormente analizaremos la calidad de los bots obtenidos para demostrar que son bots suficientemente buenos. Éstos se podrán mejorar en un futuro usando el mismo método pero con configuraciones más adecuadas para ello (como un ajuste de parámetros más fino, ejecutarlos durante más generaciones, etc).

El conjunto de parámetros considerado en nuestro algoritmo co-evolutivo de PG (Co-PG), SurvivalBot, se muestra en la Tabla \ref{tab:parameters}.
Éstos son los mismos utilizados previamente por los autores en el trabajo \cite{GarciaGP14}, GPBot, con los que se obtenían buenos bots.
Además, usamos la misma configuración porque GPBot se usará como base comparativa en los experimentos.
De modo que, para hacer más justa la comparativa, se han fijado 4000 iteraciones en SurvivalBot (2 combates por iteración), dado que en GPBot se realizaron 8000 combates (32 individuos * 5 combates por evaluación * 50 generaciones).
Se han considerado 5 mapas representativos (con configuraciones muy diferentes entre sí) que se eligen de forma aleatoria antes de cada combate.

Se han realizado 30 ejecuciones en cada caso, para obtener resultados estadísticamente significativos.


\begin{table}
\begin{center}
{\footnotesize
\begin{tabular}{|c|p{4cm}|}
\hline
{\em Nombre Parámetro} & {\em Valor} \\\hline \hline
Tamaño población & 32 \\\hline
Inicialización & Aleatoria (árboles de 3 niveles)\\ \hline
Tipo de Cruce & Cruce de sub-árbol \\ \hline
Tasa de Cruce & 0,5\\ \hline
Máximo número de turnos por combate & 1000 \\\hline
Mutación  & Mutación de 1 nodo\\ \hline
Tasa de Mutación & 0,25 \\ \hline
Selección & Justa (torneo entre 2) \\ \hline
Reemplazo & Estacionario\\ \hline
Criterio de parada & 4000 iteraciones \\ \hline
Profundidad máxima de los árboles & 7  \\ \hline
Ejecuciones & 30 \\ \hline
Mapas usados en la justa & 1 aleatorio elegido entre los mapas \#76 \#69 \#7 \#11 \#26\\ \hline
\end{tabular}
\caption{Conjunto de parámetros usado en los experimentos.} 
\label{tab:parameters}
}
\end{center}
\end{table}

% -----------------------------------------------------------------------------

\subsection{Análisis de las ejecuciones}
\label{subsec:analysisexecutions}

En este primer conjunto de experimentos analizaremos la convergencia del método propuesto, dado que se espera que su comportamiento, aún sin función explícita de fitness, sea similar al de otros AEs. El problema radica en que es difícil mostrar esta convergencia sin contar con un valor numérico para el fitness.
Por ello, una vez terminada la ejecución, se ha analizado la población de cada generación haciendo uso de una función \textit{Score}, que habíamos definido y usado en artículos anteriores para valorar el rendimiento de un bot en el juego. Ésta se basa en el desarrollo de varios combates, y la consideración del número de victorias obtenidas, turnos requeridos para ganar y turnos resistidos en caso de haber perdido.

De modo que cada individuo $i$ perteneciente a la población de cada generación/iteración se ha enfrentado con GPBot, obteniendo un Score, definido como:
 
\begin{equation}
Score_{i}=\alpha+\beta+\gamma
\label{eq:score}
\end{equation}

Donde

\begin{equation}
\alpha  = v, \alpha \in\left[0,N\right]
\end{equation}

\begin{equation}
\begin{split}
\beta =N\times\frac{t_{win}+\frac{1}{N\times t_{MAX}+1}}{\frac{t_{win}}{v+1}+1},\\
\beta \in\left[0,N\right], \\
t_{win} \in\left[0,N\times t_{MAX}\right]
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\gamma  =\frac{t_{defeated}}{N \times t_{MAX}+1}, \\
\gamma \in\left[0,1\right], \\
t_{defeated} \in\left[0,N\times t_{MAX}\right]
\end{split}
\end{equation}

Considerando el número de combates realizados ($N$), el número de victorias contra GPBot ($v$), el número total de turnos usados para ganar a GPBot ($t_{win}$), el número total de turnos aguantados, si ha sido vencido ($t_{defeated}$) y el número máximo de turnos que dura un combate ($t_{MAX} =1000$). 
Esta función tiende a favorecer las victorias frente a los turnos.
Cada individuo se ha enfrentado 3 veces en 10 mapas diferentes: los 5 usados durante la evolución, llamados \textit{mapas entrenados} y otros 5 nuevos (\textit{mapas no entrenados}), por tanto $N=30$. 

La Figura \ref{figura:Score_VS_GPBot} muestra los \textit{boxplots} del \textit{Score} obtenido por toda la población de las 30 ejecuciones en cada iteración.
Agrupados según los mapas conocidos (con los que se ha evolucionado) a la izquierda, o los mapas nuevos a la derecha. Como se puede ver la tendencia es creciente, como era deseable. Respecto a los resultados que se muestran ligeramente mejores en los mapas no conocidos (no entrenados previamente), este hecho se debe a la propiedad del algoritmo propuesto de fomentar la generalización, evitando el poco deseable efecto de sobre especialización. De forma que los bots obtenidos no ``aprenden'' a jugar en mapas específicos ni contra rivales específicos, por lo que su rendimiento contra GPBot en este caso es relativamente bueno en todos los individuos.

\begin{figure}[htb]
\tiny
\begin{center}
\includegraphics[clip=true,width=6cm]{./imags/score_vs_gpbot_training_spa.eps}
\includegraphics[clip=true,width=6cm]{./imags/score_vs_gpbot_unknown_spa.eps}
\end{center}
\caption{\textit{Score} obtenido en el enfrentamiento contra el mejor GPBot de todos los SurvivaBots obtenidos en cada generación/iteración de las 30 ejecuciones.}
\label{figura:Score_VS_GPBot}
\end{figure}

Este estudio se complementa en primer lugar con las dos gráficas que se muestran en la Figura \ref{figura:Victories_VS_GPBot_AllMaps}. En ellas se presenta el porcentaje de individuos (normalizado entre 0 y 1) que gana un determinado número de combates (de 0 a 30) contra GPBot de los que hay en la población inicial (izquierda) y en la final (derecha).
Como se puede ver, los individuos de las 30 poblaciones iniciales en las ejecuciones tienden a ganar muy pocos o ningún combate, mientras que en las poblaciones finales, el porcentaje de bots capaces de vencer la mayoría de las veces e incluso todos los combates se incrementa en gran medida. Lo que demuestra que las poblaciones, en general, han evolucionado positivamente y han conseguido bots cada vez más competitivos durante el proceso evolutivo.

\begin{figure}[htb]
\tiny
\begin{center}
  \epsfig{file=./imags/victories_vs_gpbot_allmaps_initial_spa,width=6cm}
  \epsfig{file=./imags/victories_vs_gpbot_allmaps_final_spa,width=6cm}
\end{center}
\caption{Histograma del número de victorias contra GPBot para todos los individuos de las poblaciones iniciales y finales de las 30 ejecuciones, en los 30 combates (10 mapas * 3 enfrentamientos)} 
\label{figura:Victories_VS_GPBot_AllMaps}
\end{figure}

Por último, si estudiamos la {\em edad} (número de generaciones que sobrevive un individuo) de los bots evolucionados, podremos entender mejor la dinámica del proceso. Para ello, la Figura \ref{figura:age} muestra las edades de los individuos en una ejecución. Como se puede ver, la edad media se mantiene en torno a un valor estable durante toda la ejecución, lo que nos indica que la población está continuamente mejorando, de modo que los hijos son capaces de vencer a sus padres en pocas generaciones. Existen valores extremos, debidos, sin duda, al componente aleatorio que hay en la selección de los padres, por el que algunos individuos combatirán muy pocas veces (o ninguna) en una ejecución.
%
\begin{figure}[htb]
\tiny
\begin{center}
  \epsfig{file=./imags/ageall_spa,width=5cm}
\end{center}
\caption{\textit{Boxplots} de la edad (número de generaciones/iteraciones que sobrevive un bot) de la población en una ejecución.}

\label{figura:age}
\end{figure}
%

A tenor de lo visto en estos experimentos, consideramos que la evolución del algoritmo es la adecuada. Pasaremos a analizar la influencia del ruido en este proceso.

% ---------------------------------------------------------------------------

\subsection{Análisis de ruido}
\label{subsec:analysisnoise}

En este estudio se compararán los mejores 30 bots obtenidos por GPBot con los mejores 30 SurvivalBots (de las 30 ejecuciones). 
Para elegir el mejor SurvivalBot de cada ejecución, se ha realizado un torneo \textit{todos contra todos} entre los bots de la última generación. El bot que ha ganado más combates ha sido considerado como el mejor. Hemos aplicado este método para evitar el uso de una función Score sobre bots evolucionados sin fitness, a fin de evitar las desventajas que esta función conlleva, como hemos explicado a lo largo del artículo.

Éstos se han enfrentado contra el mejor rival de nuestros trabajos, el bot experto o especializado ExpGeneBot \cite{Genebot_CIG2012}, capaz de adaptar sus estrategias a las características del mapa en el que se desarrolla el combate. 
Dichos enfrentamientos se han realizado en los mismos 10 mapas considerados en el experimento anterior, en cada uno de los cuales se han realizado 30 combates y calculado el Score de los bots implicados, siguiendo la Ecuación \ref{eq:score}.

A fin de comprobar la influencia que ha tenido el ruido en la generación de los SurvivalBots, hemos calculado un \textit{factor de ruido} para cada bot, el cual se ha obtenido como la diferencia entre el máximo y el mínimo de los Scores obtenidos en los 30 combates que ha llevado a cabo cada bot en cada mapa. 

La Figura \ref{figura:noise} muestra los \textit{boxplots} de los 30 GPBots y SurvivalBots. Según la definición del factor de ruido que hemos propuesto, una distancia grande entre valores significará una mayor incertidumbre en los resultados.
Las gráficas muestran que los SurvivalBots se comportan mejor en este sentido, presentando una varianza menor en resultados contra un rival de gran dificultad y capaz de autoadaptar su comportamiento en función del mapa y la partida. De modo que se puede concluir que los bots obtenidos mediante el método descrito en este artículo son más fiables o ``robustos'' en términos de comportamiento y rendimiento.
%
\begin{figure}[htb]
\tiny
\begin{center}
  \epsfig{file=./imags/noise_study_spa,width=4.5cm}
\end{center}
\caption{{\em Factor de ruido}, calculado como el máximo score obtenido - el mínimo en cada mapa, para cada uno de los 30 GPBots y SurvivalBots. 30 combates por mapa y 10 mapas diferentes.}
\label{figura:noise}
\end{figure}
%

%\vspace{-0.7cm}
Finalmente, en la siguiente sección analizaremos la calidad o competitividad de los bots obtenidos con el método propuesto, ya que ese es el objetivo final de esta evolución.

% --------------------------------------------------------------

\subsection{Análisis de los bots generados}
\label{subsec:analysisbots}

En este experimento todos los SurvivalBots obtenidos al final de las ejecuciones se han enfrentado a otros bots del estado del arte. Para ello, en primer lugar se han seleccionado los 30 mejores, como se hizo en el experimento anterior.
Dichos bots han sido enfrentados contra un bot básico (BullyBot) y 4 del estado del arte. Los bots y sus configuraciones se pueden ver en la Tabla \ref{tab:literaturebots}.

Los enfrentamientos se han realizado en los 100 mapas de ejemplo que proporcionó Google en el campeonato, los cuales no han sido usados durante la evolución. 

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
{\em Bot} & {Referencia} & {\em Combates en el entrenamiento} & {\em Max. Turnos} \\\hline \hline
BullyBot & Web del Google AI Challenge & No & No \\ \hline
SurvivalBot & propuesto aquí & 8000 & 1000 \\ \hline
GeneBot & \cite{Genebot_JCST} & 32000 & 1000 \\ \hline
ExpGeneBot & \cite{Genebot_CIG2012} & 32000 & 1000 \\ \hline
GPBot & \cite{GarciaGP14} & 8000 & 1000 \\ \hline
HoFBot & \cite{NogueiraCoevolutionary14} & 180000 & 500 \\ \hline
\end{tabular}
\caption{Bots disponibles en la literatura que hemos usado para medir la calidad de los SurvivalBots}
\label{tab:literaturebots}
\end{center}
\end{table}

%\vspace{-1.2cm}
La Figura \ref{figure:boxplot_mejores_contra_clasicos} muestra los \textit{boxplots} del porcentaje de victorias obtenido por los SurvivalBots contra los demás rivales (no se consideran los empates).

El resultado más interesante es que GPBot es ampliamente superado, dado que el número de combates en la evolución es el mismo y la base de sus motores de IA, basada en PG es similar.
El HoFBot, que también se obtuvo usando un algoritmo co-evolutivo, ha sido vencido más del 50\% de los combates por la mayoría de los mejores SurvivalBots.
Sin embargo, bots ampliamente entrenados (4 veces más evaluaciones) y basados en una IA diseñada por un experto, como son GeneBot y ExpGeneBot, han resultado difíciles de vencer, como era de esperar. 

\begin{figure}[htb]
\tiny
\begin{center}
    \includegraphics[width=6cm]{./imags/boxplot_mejores_contra_clasicos_spa.eps}

\end{center}
\caption{Porcentaje de victorias al enfrentar los mejores 30 SurvivalBots contra otros bots de la literatura.}
\label{figure:boxplot_mejores_contra_clasicos}
\end{figure}

%\vspace{-0.5cm}
En cualquier caso, este es un punto de partida para SurvivalBot, que tiene mucho margen de mejora, empezando por un mayor número de iteraciones/combates en su entrenamiento, que llevarían a obtener bots más competitivos.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  CONCLUSIONS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Conclusiones y trabajo futuro}
\label{sec:conclusions}

Este artículo presenta una implementación de un algoritmo de Programación Genética (PG) co-evolutivo simple para la generación de bots (NPCs) para RTSs. Éste método propone la omisión de la selección basada en fitness en el proceso evolutivo. En su lugar, se simula una evolución de forma más natural, puesto que se basa en la mera supervivencia de los individuos, que se enfrentan en combates contra otros en los que sólo el ganador pasa a la siguiente generación.

Esta propuesta se ha aplicado a la creación y mejora de reglas de comportamiento
para la IA de un bot que juegue a Planet Wars.
De forma que la selección se ha modelado como in combate en el juego, que hemos denominado \textit{justa}, en el que el ganador será padre de la siguiente descendencia, mientras que el perdedor será reemplazado por dicha descendencia.

Según se ha demostrado en los experimentos, el algoritmo propuesto ofrece tres beneficios principales: 
\begin{itemize}
\item Genera bots más generalistas en su comportamiento, no especializados en un rival en concreto, dado que no utiliza rivales específicos en sus evaluaciones, sino miembros de la propia población.
\item Se ve menos afectado por el ruido intrínseco a las funciones de evaluación en el entorno de los videojuegos, debido a la gran presión selectiva que introduce el hecho de que un sólo combate perdido por un bot haría que éste se eliminara de la población. De modo que es muy difícil que bots que no sean realmente buenos sobrevivan.
\item Requiere menos combates en la evaluación (sólo uno), en contraposición a los múltiples combates que se suelen realizar en las evaluaciones de otras propuestas. Esto hace que se reduzca el tiempo de cómputo del algoritmo.
\end{itemize}

Los bots obtenidos, llamados SurvivalBots, han sido enfrentados contra otros bots del estado del arte, obteniendo muy buenos resultados en general, considerando que el menor entrenamiento de los bots obtenidos en esta propuesta.

Como líneas de trabajo futuro se plantean muchas, ya que este enfoque es reciente en nuestras investigaciones. En primer lugar nos centraremos en obtener bots más competitivos y complejos, añadiendo más variedad de condiciones y acciones al algoritmo de PG, como por ejemplo distancias entre planetas.
En la misma línea se harán pruebas sin limitar el tamaño máximo de los árboles.

\section*{Agradecimientos}

Este trabajo ha sido financiado en parte por los proyectos: 
EPHEMECH (TIN2014-56494-C4-3-P) y KNOWAVES (TEC2015-68752-R), Ministerio de Economía y Competitividad, Fondos FEDER, PROY-PP2015-06 (Plan Propio 2015 UGR), 
y el proyecto MOSOS (PRY142/14), financiado por la Fundación Pública Andaluza Centro de Estudios Andaluces en la IX Convocatoria de Proyectos de Investigación.

\bibliographystyle{splncs03}
\bibliography{survival_gpbot}



\end{document}
